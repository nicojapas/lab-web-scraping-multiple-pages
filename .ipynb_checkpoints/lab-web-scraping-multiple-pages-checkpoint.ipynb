{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pretty-mystery",
   "metadata": {},
   "source": [
    "![logo_ironhack_blue 7](https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png)\n",
    "\n",
    "# Lab | Web Scraping Multiple Pages\n",
    "\n",
    "#### Business goal:\n",
    "\n",
    "- Check the `case_study_gnod.md` file.\n",
    "- Make sure you've understood the big picture of your project:\n",
    "\n",
    "  - the goal of the company (`Gnod`),\n",
    "  - their current product (`Gnoosic`),\n",
    "  - their strategy, and\n",
    "  - how your project fits into this context.\n",
    "\n",
    "  Re-read the business case and the e-mail from the CTO, take a look at the flowchart and create an initial Trello board with the tasks you think you'll have to accomplish.\n",
    "\n",
    "#### Instructions \n",
    "\n",
    "#### Prioritize the MVP\n",
    "\n",
    "In the previous lab, you had to scrape data about \"hot songs\". It's critical to be on track with that part, as it was part of the request from the CTO.\n",
    "\n",
    "If you couldn't finish the first lab, use this time to go back there.\n",
    "\n",
    "#### Expand the project\n",
    "\n",
    "If you're done, you can try to expand the project on your own. Here are a few suggestions:\n",
    "\n",
    "- Find other lists of hot songs on the internet and scrape them too: having a bigger pool of songs will be awesome!\n",
    "- Apply the same logic to other \"groups\" of songs: the best songs from a decade or from a country / culture / language / genre.\n",
    "- Wikipedia maintains a large collection of lists of songs: https://en.wikipedia.org/wiki/Lists_of_songs\n",
    "\n",
    "#### Practice web scraping\n",
    "\n",
    "As you've seen, scraping the internet is a skill that can get you all sorts of information. Here are some little challenges that you can try to gain more experience in the field:\n",
    "\n",
    "- Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: `url ='https://en.wikipedia.org/wiki/Python'`\n",
    "- Find the number of titles that have changed in the United States Code since its last release point: `url = 'http://uscode.house.gov/download/download.shtml'`\n",
    "- Create a Python list with the top ten FBI's Most Wanted names: `url = 'https://www.fbi.gov/wanted/topten'`\n",
    "- Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: `url = 'https://www.emsc-csem.org/Earthquake/'`\n",
    "- List all language names and number of related articles in the order they appear in [wikipedia.org](wikipedia.org): `url = 'https://www.wikipedia.org/'`\n",
    "- A list with the different kind of datasets available in [data.gov.uk](data.gov.uk): `url = 'https://data.gov.uk/'`\n",
    "- Display the top 10 languages by number of native speakers stored in a pandas dataframe: `url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blind-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspected-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.billboard.com/charts/hot-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "explicit-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rational-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "artists = []\n",
    "for tag in soup.find_all(\"span\", attrs={\"class\":\"chart-element__information__song text--truncate color--primary\"}):\n",
    "    titles.append(tag.get_text())\n",
    "for tag in soup.find_all(\"span\", attrs={\"class\":\"chart-element__information__artist text--truncate color--secondary\"}):\n",
    "    artists.append(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collected-insured",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drivers License</td>\n",
       "      <td>Olivia Rodrigo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34+35</td>\n",
       "      <td>Ariana Grande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calling My Phone</td>\n",
       "      <td>Lil Tjay Featuring 6LACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blinding Lights</td>\n",
       "      <td>The Weeknd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Up</td>\n",
       "      <td>Cardi B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                    artist\n",
       "0   Drivers License            Olivia Rodrigo\n",
       "1             34+35             Ariana Grande\n",
       "2  Calling My Phone  Lil Tjay Featuring 6LACK\n",
       "3   Blinding Lights                The Weeknd\n",
       "4                Up                   Cardi B"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'title':titles, 'artist':artists})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-frequency",
   "metadata": {},
   "source": [
    "Get more songs from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liable-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(1,7):\n",
    "    urls.append(f\"https://en.wikipedia.org/wiki/List_of_songs_in_Glee_(season_{i})\")\n",
    "\n",
    "response = requests.get(urls[0])\n",
    "\n",
    "soups = []\n",
    "for i in urls:\n",
    "    soups.append(BeautifulSoup((requests.get(i)).content, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soups[0].select('.wikitable > tbody > tr > th > a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "through-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "artists = []\n",
    "\n",
    "for i in soups:\n",
    "    for tag in (i.select('.wikitable > tbody > tr > th:nth-child(1)')):\n",
    "        if (tag['scope'] == 'row'):\n",
    "            titles.append(tag.get_text().rstrip().strip('\\\"'))\n",
    "    for tag in i.select('.wikitable > tbody > tr > td:nth-child(3)'):\n",
    "        artists.append(tag.get_text())\n",
    "#     print('titles',len(titles),'artists',len(artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "educated-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_data(df, titles, artists):\n",
    "    df = pd.concat([df, pd.DataFrame({'title':titles, 'artist':artists})], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "forward-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_new_data(data, titles, artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bronze-burst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(843, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-quantity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "western-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 40 'alt' songs. Can be searched by week (yyyy-mm-dd)\n",
    "# Let's get one year of data\n",
    "urls = []\n",
    "dates = pd.date_range('2020-02-24', '2021-02-24', freq='W')\n",
    "dates = [date.strftime('%Y-%m-%d') for date in dates]\n",
    "\n",
    "for i in dates:\n",
    "    urls.append(f'https://www.billboard.com/charts/alternative-airplay/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "norwegian-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b297e535d70448aaa3a3db84d301ab70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soups = []\n",
    "for url in tqdm(urls):\n",
    "    soups.append(BeautifulSoup(requests.get(url).content))\n",
    "    sleep(random.random()*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intended-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "artists = []\n",
    "\n",
    "for soup in soups:\n",
    "\n",
    "    for tag in soup.find_all('span', 'chart-list-item__title-text'):\n",
    "        titles.append(tag.get_text().strip())\n",
    "    \n",
    "    for tag in soup.find_all('div', 'chart-list-item__artist'):\n",
    "        artists.append(tag.get_text().strip())\n",
    "\n",
    "#     print(len(titles), len(artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "multiple-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_new_data(data, titles, artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instant-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1923, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-wisconsin",
   "metadata": {},
   "source": [
    "Recommendation algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unnecessary-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_song():\n",
    "    print(\"\"\"\n",
    "    Enter your song\n",
    "    example:\n",
    "    >>> Where Is Love?\n",
    "    \"\"\")\n",
    "    user_input = input()\n",
    "    \n",
    "    dtemp = data['title'][data['title'].str.contains(user_input, na=False, case=False)]\n",
    "    dtemp = dtemp.to_frame().reset_index(drop=True)\n",
    "    print(dtemp)\n",
    "    print(\"\"\"\n",
    "    Is it any of this?\n",
    "    Pick one\n",
    "    \"\"\")\n",
    "    user_input = int(input())\n",
    "    \n",
    "    print(\"\"\"\n",
    "    You selected:\n",
    "    \"\"\")\n",
    "    print(data[data['title'] == dtemp.iloc[user_input].title])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "incredible-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendation(x):\n",
    "    print(\"\"\"\n",
    "    Making recommendation\n",
    "    \"\"\")\n",
    "    [i for i in tqdm(range(10000000))]\n",
    "    y = data.sample()\n",
    "    \n",
    "    print(\"\"\"\n",
    "    The almighty algorithm recommends:\n",
    "    \"\"\")\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "creative-asbestos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Enter your song\n",
      "    example:\n",
      "    >>> Where Is Love?\n",
      "    \n",
      "where\n",
      "                    title\n",
      "0          Where Is Love?\n",
      "1  Somewhere Only We Know\n",
      "2               Somewhere\n",
      "3  I Know Where I've Been\n",
      "\n",
      "    Is it any of this?\n",
      "    Pick one\n",
      "    \n",
      "2\n",
      "\n",
      "    You selected:\n",
      "    \n",
      "         title                            artist\n",
      "276  Somewhere  Rachel Berry and Shelby Corcoran\n",
      "\n",
      "    Making recommendation\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b78b07f3d2242849128412083808a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The almighty algorithm recommends:\n",
      "    \n",
      "      title artist\n",
      "1009  Bang!    AJR\n"
     ]
    }
   ],
   "source": [
    "print(make_recommendation(ask_song()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-peninsula",
   "metadata": {},
   "source": [
    "- Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: `url ='https://en.wikipedia.org/wiki/Python'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "backed-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/Python'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "conditional-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Pythons\n",
      "https://en.wikipedia.org/wiki/Python_(genus)\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/Python_of_Aenus\n",
      "https://en.wikipedia.org/wiki/Python_(Efteling)\n",
      "https://en.wikipedia.org/wiki/Python_(automobile_maker)\n",
      "https://en.wikipedia.org/wiki/Python_(missile)\n",
      "https://en.wikipedia.org/wiki/PYTHON\n",
      "https://en.wikipedia.org/wiki/Python_(Monty)_Pictures\n",
      "https://en.wikipedia.org/wiki/Cython\n"
     ]
    }
   ],
   "source": [
    "WIKI_URL = 'https://en.wikipedia.org'\n",
    "\n",
    "for tag in soup.find('div', 'mw-parser-output').find_all('ul'):\n",
    "    if tag.a.get('href')[0] == \"/\":\n",
    "        print(WIKI_URL+tag.a.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-interference",
   "metadata": {},
   "source": [
    "- Find the number of titles that have changed in the United States Code since its last release point: `url = 'http://uscode.house.gov/download/download.shtml'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "integrated-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "humanitarian-underground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The are 13 titles changed.\n"
     ]
    }
   ],
   "source": [
    "print(\"The are \"+str(len(soup.find_all('div', 'usctitlechanged')))+\" titles changed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "linear-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 5 - Government Organization and Employees ٭\n",
      "Title 6 - Domestic Security\n",
      "Title 20 - Education\n",
      "Title 25 - Indians\n",
      "Title 29 - Labor\n",
      "Title 33 - Navigation and Navigable Waters\n",
      "Title 34 - Crime Control and Law Enforcement\n",
      "Title 36 - Patriotic and National Observances, Ceremonies, and Organizations ٭\n",
      "Title 38 - Veterans' Benefits ٭\n",
      "Title 40 - Public Buildings, Property, and Works ٭\n",
      "Title 42 - The Public Health and Welfare\n",
      "Title 49 - Transportation ٭\n",
      "Title 51 - National and Commercial Space Programs ٭\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all('div', 'usctitlechanged'):\n",
    "    print(tag.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-garage",
   "metadata": {},
   "source": [
    "- Create a Python list with the top ten FBI's Most Wanted names: `url = 'https://www.fbi.gov/wanted/topten'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "selected-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "removable-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.get_text().strip() for i in soup.find_all('h3', 'title')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-tooth",
   "metadata": {},
   "source": [
    "- Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: `url = 'https://www.emsc-csem.org/Earthquake/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "finished-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "unavailable-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "time = []\n",
    "latitude = []\n",
    "longitude = []\n",
    "region_name = []\n",
    "\n",
    "for tag1 in soup.find(id='tbody').find_all('tr', limit=10):\n",
    "    s = tag1.get_text().split('earthquake')[1]\n",
    "    date.append(s.split()[0])\n",
    "    time.append(s.split()[1][:10])\n",
    "    v = s.split(' ago')[1].split()[0:2]\n",
    "    latitude.append(\"\".join(v)[:-1]+\" \"+\"\".join(v)[-1])\n",
    "    v = s.split(' ago')[1].split()[2:4]\n",
    "    longitude.append(\"\".join(v)[:-1]+\" \"+\"\".join(v)[-1])\n",
    "    region_name.append(tag1.find('td', 'tb_region').string.replace(u'\\xa0', u''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "solid-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>10:05:33.0</td>\n",
       "      <td>16.41 N</td>\n",
       "      <td>98.47 W</td>\n",
       "      <td>GUERRERO, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>09:56:04.8</td>\n",
       "      <td>38.69 N</td>\n",
       "      <td>15.69 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>09:48:21.3</td>\n",
       "      <td>39.30 N</td>\n",
       "      <td>41.15 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>09:22:02.0</td>\n",
       "      <td>24.17 S</td>\n",
       "      <td>67.43 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:51:05.0</td>\n",
       "      <td>47.48 S</td>\n",
       "      <td>100.11 E</td>\n",
       "      <td>SOUTHEAST INDIAN RIDGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:44:29.0</td>\n",
       "      <td>9.30 N</td>\n",
       "      <td>123.18 E</td>\n",
       "      <td>NEGROS- CEBU REG, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:41:09.3</td>\n",
       "      <td>38.18 N</td>\n",
       "      <td>117.79 W</td>\n",
       "      <td>NEVADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:37:54.0</td>\n",
       "      <td>0.31 N</td>\n",
       "      <td>98.59 E</td>\n",
       "      <td>NIAS REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:36:39.3</td>\n",
       "      <td>36.63 N</td>\n",
       "      <td>71.49 E</td>\n",
       "      <td>HINDU KUSH REGION, AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>08:11:51.7</td>\n",
       "      <td>36.71 N</td>\n",
       "      <td>121.35 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        time latitude longitude                     region_name\n",
       "0  2021-02-25  10:05:33.0  16.41 N   98.47 W                GUERRERO, MEXICO\n",
       "1  2021-02-25  09:56:04.8  38.69 N   15.69 E                   SICILY, ITALY\n",
       "2  2021-02-25  09:48:21.3  39.30 N   41.15 E                  EASTERN TURKEY\n",
       "3  2021-02-25  09:22:02.0  24.17 S   67.43 W                SALTA, ARGENTINA\n",
       "4  2021-02-25  08:51:05.0  47.48 S  100.11 E          SOUTHEAST INDIAN RIDGE\n",
       "5  2021-02-25  08:44:29.0   9.30 N  123.18 E   NEGROS- CEBU REG, PHILIPPINES\n",
       "6  2021-02-25  08:41:09.3  38.18 N  117.79 W                          NEVADA\n",
       "7  2021-02-25  08:37:54.0   0.31 N   98.59 E          NIAS REGION, INDONESIA\n",
       "8  2021-02-25  08:36:39.3  36.63 N   71.49 E  HINDU KUSH REGION, AFGHANISTAN\n",
       "9  2021-02-25  08:11:51.7  36.71 N  121.35 W              CENTRAL CALIFORNIA"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'date':date, 'time':time, 'latitude':latitude, 'longitude':longitude, 'region_name':region_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-raising",
   "metadata": {},
   "source": [
    "- List all language names and number of related articles in the order they appear in [wikipedia.org](wikipedia.org): `url = 'https://www.wikipedia.org/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "embedded-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.wikipedia.org/'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "organic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['lang'+str(i) for i in range(1,11)]\n",
    "results = []\n",
    "\n",
    "for lang in langs:\n",
    "    results.append(soup.find('div', lang).get_text().split()[0])\n",
    "    results.append(soup.find('div', lang).get_text().split(maxsplit=1)[1].replace('\\xa0','').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial-vampire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>6245000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1252000+ 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2534000+ Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Español</td>\n",
       "      <td>1659000+ artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1697000+ статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2296000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1672000+ voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1175000+ 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1454000+ haseł</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Português</td>\n",
       "      <td>1055000+ artigos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language         description\n",
       "0    English   6245000+ articles\n",
       "1        日本語         1252000+ 記事\n",
       "2    Deutsch    2534000+ Artikel\n",
       "3    Español  1659000+ artículos\n",
       "4    Русский     1697000+ статей\n",
       "5   Français   2296000+ articles\n",
       "6   Italiano       1672000+ voci\n",
       "7         中文         1175000+ 條目\n",
       "8     Polski      1454000+ haseł\n",
       "9  Português    1055000+ artigos"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'language':results[::2], 'description':results[1:][::2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-insight",
   "metadata": {},
   "source": [
    "- A list with the different kind of datasets available in [data.gov.uk](data.gov.uk): `url = 'https://data.gov.uk/'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "timely-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.data.gov.uk/'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "broadband-madagascar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tag.a.string for tag in soup.find_all('h3', 'govuk-heading-s dgu-topics__heading')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-explanation",
   "metadata": {},
   "source": [
    "- Display the top 10 languages by number of native speakers stored in a pandas dataframe: `url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "referenced-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "remarkable-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(1,11):\n",
    "    l.append(soup.select('.mw-parser-output > .wikitable > tbody')[0].find_all('tr')[i].a.string)\n",
    "    l.append(soup.select('.mw-parser-output > .wikitable > tbody')[0].find_all('tr')[i].find_all('td')[2].string.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "following-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>speakers(millons)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           language speakers(millons)\n",
       "0  Mandarin Chinese               918\n",
       "1           Spanish               480\n",
       "2           English               379\n",
       "3             Hindi               341\n",
       "4           Bengali               228\n",
       "5        Portuguese               221\n",
       "6           Russian               154\n",
       "7          Japanese               128\n",
       "8   Western Punjabi              92.7\n",
       "9           Marathi              83.1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'language':l[::2], 'speakers(millons)':l[1:][::2]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
